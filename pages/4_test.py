# app.py
# Streamlit é¡µé¢ï¼šæŒ‰æ—¶é—´åŠ¨æ€åœ¨åœ°å›¾ä¸Šå±•ç¤ºå„ç«™ç‚¹æŒ¯å¹…çš„ 3D æŸ±çŠ¶å›¾ï¼ˆæ¯åˆ†é’Ÿå‡åŒ€30å¸§ï¼‰
from session_fix import page_setup
import io
import os
import re
import time
import requests
import numpy as np
import pandas as pd
import streamlit as st
import pydeck as pdk
from obspy import read

# ==============================
# 1) å…¨å±€ URL åˆ—è¡¨ï¼ˆè¯·æ›¿æ¢ä¸ºä½ çš„å¯è®¿é—® mseed é“¾æ¥ï¼‰
#    æ–‡ä»¶åå¿…é¡»å½¢å¦‚ï¼šIU_AFI_-13.91_-171.78_waveforms.mseed
# ==============================

from integration_helper import check_authentication, add_auth_sidebar
user_info = check_authentication()
add_auth_sidebar()

from typing import List, Tuple, Optional
from urllib.parse import urlparse, quote, unquote

from obs import ObsClient  # pip install esdk-obs-python

# -----------------------
# è¯·åœ¨æ­¤å¤„é…ç½®æˆ–ä»ç¯å¢ƒè¯»å–
# -----------------------
ENDPOINT = "https://obs.cn-north-4.myhuaweicloud.com"
BUCKET = "gaoyuan-49d0"
PREFIX = "åœ°è´¨ç¾å®³-æ–‡æœ¬+é¥æ„Ÿ+æ—¶åº/IRIS/dataset_earthquake/" 

# # å¦‚æœéœ€è¦å‡­è¯åˆ™å¡«å†™ï¼Œè‹¥ OBS å…¬å¼€å¯è®¿é—®å¯ç•™ç©ºå¹¶åªä¼  serverï¼ˆæˆ–ä½¿ç”¨ ENV æœºåˆ¶ï¼‰
# ACCESS_KEY = None  # æˆ– "ä½ çš„AK"
# SECRET_KEY = None  # æˆ– "ä½ çš„SK"

# -----------------------
# å†…éƒ¨å®ç°
# -----------------------

def _create_obs_client(endpoint: str, access_key: Optional[str], secret_key: Optional[str]) -> ObsClient:
    """åˆ›å»º ObsClientï¼šè‹¥æä¾› AK/SK åˆ™ä½¿ç”¨ï¼Œå¦åˆ™åªä¼  serverï¼ˆå¯å€ŸåŠ© ENV/ECS ç­–ç•¥ï¼‰ã€‚"""
    if access_key and secret_key:
        return ObsClient(access_key_id=access_key, secret_access_key=secret_key, server=endpoint)
    else:
        # å¦‚æœç¯å¢ƒå˜é‡æˆ– ECS æˆæƒå¯ç”¨ï¼Œä¸‹é¢çš„æ„é€ ä¹Ÿèƒ½å·¥ä½œï¼ˆå‚è€ƒå®˜æ–¹ security_provider_policy é€‰é¡¹ï¼‰
        return ObsClient(server=endpoint)


def list_all_objects_under_prefix(client: ObsClient, bucket: str, prefix: str, max_keys: int = 1000) -> List[str]:
    """
    ä½¿ç”¨ listObjects + marker åˆ†é¡µè·å– prefix ä¸‹çš„æ‰€æœ‰ object keysï¼ˆé€’å½’ï¼‰ã€‚
    è¿”å› object key åˆ—è¡¨ï¼ˆobject nameï¼Œä¸å« bucketï¼‰ã€‚
    æ³¨æ„ï¼šæ¯æ¬¡æœ€å¤š max_keysï¼ˆä¸Šé™ 1000ï¼‰ã€‚
    å‚è€ƒæ–‡æ¡£ï¼šlistObjects / is_truncated / next_markerã€‚:contentReference[oaicite:3]{index=3}
    """
    # ä¿è¯ prefix æ˜¯å¯¹è±¡åå½¢å¼ï¼ˆé URL ç¼–ç ï¼‰
    prefix = prefix.lstrip('/')
    keys = []
    marker = None

    while True:
        resp = client.listObjects(bucket, prefix=prefix, marker=marker, max_keys=max_keys)
        if resp.status >= 300:
            raise RuntimeError(f"listObjects failed: status={resp.status}, reason={resp.reason}, msg={getattr(resp, 'errorMessage', '')}")

        body = resp.body
        contents = getattr(body, "contents", []) or []
        for c in contents:
            # æ”¯æŒå±æ€§è®¿é—®æˆ–å­—å…¸è®¿é—®ï¼ˆå…¼å®¹ SDK è¿”å›æ ¼å¼ï¼‰
            key = getattr(c, "key", None) or (c.get("key") if isinstance(c, dict) else None)
            if key:
                keys.append(key)

        # ç¿»é¡µåˆ¤æ–­
        if not getattr(body, "is_truncated", False):
            break

        # next marker
        next_marker = getattr(body, "next_marker", None) or getattr(body, "nextMarker", None)
        if next_marker:
            marker = next_marker
        else:
            # å…¼å®¹å¤„ç†ï¼šä½¿ç”¨æœ€åä¸€ä¸ª key çš„å€¼
            if contents:
                last_key = None
                last = contents[-1]
                last_key = getattr(last, "key", None) or (last.get("key") if isinstance(last, dict) else None)
                if last_key:
                    marker = last_key
                else:
                    break
            else:
                break

    return keys


def filter_and_dedup_mseed(keys: List[str], prefix: str, max_depth: Optional[int] = None) -> List[str]:
    """
    ä» keysï¼ˆå®Œæ•´ object key åˆ—è¡¨ï¼‰ç­›é€‰ .mseed æ–‡ä»¶ï¼Œå¹¶æŒ‰è§„åˆ™å»é‡ï¼š
    - å¿½ç•¥ depth è¶…å‡º max_depth çš„æ–‡ä»¶ï¼ˆè‹¥æä¾›ï¼‰
    - å¯¹äºç±»ä¼¼ `xx.mseed`, `xx_1.mseed`, `xx_2.mseed` çš„ç»„ï¼Œåªä¿ç•™ first choice:
        * ä¼˜å…ˆé€‰æ‹©æ²¡æœ‰ _æ•°å­— çš„åŸå `xx.mseed`ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        * å¦åˆ™é€‰æ‹©è¯¥ç»„ä¸­æ’åºæœ€é å‰çš„ä¸€ä¸ª
    è¿”å›é€‰ä¸­çš„ object keysï¼ˆç›¸å¯¹äº bucketï¼‰ã€‚
    """
    prefix = prefix.rstrip('/') + '/' if prefix and not prefix.endswith('/') else prefix
    mseed_keys = []
    for k in keys:
        if not k.lower().endswith('.mseed'):
            continue
        # depth é™åˆ¶ï¼ˆç›¸å¯¹äº prefix çš„ç›¸å¯¹è·¯å¾„ï¼‰
        if prefix and k.startswith(prefix):
            rel = k[len(prefix):]
        else:
            rel = k
        if max_depth is not None:
            # å¦‚æœ rel ä¸ºç©ºæˆ–ä»¥ / ç»“å°¾ï¼Œè®¡ç®—æ·±åº¦ä¸º segments æ•°
            depth = 0 if rel == "" else rel.count('/')
            if depth > max_depth:
                continue
        mseed_keys.append(k)

    # group by stem: å»æ‰ç»“å°¾çš„ "_æ•°å­—"ï¼ˆä»…åœ¨ .mseed æ‰©å±•åå‰ï¼‰
    groups = {}
    for k in mseed_keys:
        fname = os.path.basename(k)
        # stem å»é™¤ _1,_2 å½¢å¼çš„å°¾å·ï¼ˆä»…æœ€åä¸€æ®µä¸ºæ•°å­—æ—¶ï¼‰
        stem = re.sub(r'(_\d+)(?=\.mseed$)', '', fname, flags=re.IGNORECASE)
        groups.setdefault(stem, []).append(k)

    selected = []
    for stem, lst in groups.items():
        # ä¼˜å…ˆæ‰¾çº¯å stem + ".mseed"
        plain = stem if stem.lower().endswith('.mseed') else f"{stem}.mseed"
        exact = None
        for k in lst:
            if os.path.basename(k) == plain:
                exact = k
                break
        if exact:
            chosen = exact
        else:
            chosen = sorted(lst)[0]  # å¦åˆ™é€‰æ‹©å­—å…¸åºæœ€å…ˆçš„åšä»£è¡¨
        selected.append(chosen)

    # è¿”å›æ’åºè¿‡çš„ç»“æœï¼ˆä¾¿äºç¨³å®šï¼‰
    return sorted(selected)


def build_public_urls(endpoint: str, bucket: str, keys: List[str]) -> List[str]:
    """
    æ ¹æ® endpoint ä¸ bucket æ„é€ å…¬å¼€è®¿é—® URL åˆ—è¡¨ã€‚
    - è‹¥ endpoint ä¸»æœºåå·²åŒ…å« bucketï¼ˆä¾‹å¦‚ user ç»™çš„æ˜¯ 'https://gaoyuan-49d0.obs.cn-north-4.myhuaweicloud.com'ï¼‰ï¼Œç›´æ¥ç”¨è¯¥ endpoint ä½œä¸º baseï¼›
    - å¦åˆ™æŒ‰ç…§ {scheme}://{bucket}.{netloc} æ„é€  baseã€‚
    å¯¹ object key åš urllib.parse.quote ç¼–ç ï¼ˆä¿ç•™ '/').
    """
    endpoint = endpoint.rstrip('/')
    parsed = urlparse(endpoint if '://' in endpoint else 'https://' + endpoint)
    scheme = parsed.scheme or 'https'
    netloc = parsed.netloc or parsed.path

    if netloc.startswith(bucket + "."):
        base = f"{scheme}://{netloc}"
    else:
        base = f"{scheme}://{bucket}.{netloc}"

    urls = [f"{base}/{quote(k, safe='/')}" for k in keys]
    return urls


def build_mseed_url_list(
    endpoint: str,
    bucket: str,
    prefix: str,
    access_key: Optional[str] = None,
    secret_key: Optional[str] = None,
    max_depth: Optional[int] = None,
) -> List[str]:
    """
    é«˜å±‚å‡½æ•°ï¼šåˆ—å‡º prefix ä¸‹çš„æ‰€æœ‰ mseedï¼ˆå»é‡ï¼‰ï¼Œè¿”å›å¯è®¿é—® URL åˆ—è¡¨ã€‚
    """
    # ç¡®ä¿ prefix æ˜¯è§£ç åçš„è·¯å¾„ï¼ˆè‹¥ç”¨æˆ·ä¸æ…ä¼ äº† URL ç¼–ç ï¼Œå…ˆè§£ç ï¼‰
    prefix = unquote(prefix)
    client = _create_obs_client(endpoint, access_key, secret_key)
    try:
        keys = list_all_objects_under_prefix(client, bucket, prefix)
        mseed_keys = filter_and_dedup_mseed(keys, prefix, max_depth=max_depth)
        urls = build_public_urls(endpoint, bucket, mseed_keys)
        return urls
    finally:
        try:
            client.close()
        except Exception:
            pass

# url_list = [
#     "https://gaoyuan-49d0.obs.cn-north-4.myhuaweicloud.com/%E5%9C%B0%E8%B4%A8%E7%81%BE%E5%AE%B3-%E6%96%87%E6%9C%AC%2B%E9%81%A5%E6%84%9F%2B%E6%97%B6%E5%BA%8F/IRIS/dataset_earthquake/IU_AFI_-13.91_-171.78_waveforms.mseed",
#     "https://gaoyuan-49d0.obs.cn-north-4.myhuaweicloud.com/%E5%9C%B0%E8%B4%A8%E7%81%BE%E5%AE%B3-%E6%96%87%E6%9C%AC%2B%E9%81%A5%E6%84%9F%2B%E6%97%B6%E5%BA%8F/IRIS/dataset_earthquake/IU_CASY_-66.28_110.54_waveforms.mseed",
# ]
url_list = build_mseed_url_list(ENDPOINT, BUCKET, PREFIX, None, None, max_depth=None)

# ------------------------------
# å·¥å…·å‡½æ•°
# ------------------------------
FNAME_RE = re.compile(
    r"(?P<net>[A-Z0-9]+)_(?P<sta>[A-Z0-9]+)_(?P<lat>[-0-9.]+)_(?P<lon>[-0-9.]+)_waveforms\.mseed$"
)

def parse_meta_from_url(url: str):
    """ä»æ–‡ä»¶åè§£æç½‘ç»œã€å°ç«™ã€ç»çº¬åº¦"""
    fname = os.path.basename(url)
    m = FNAME_RE.match(fname)
    if not m:
        raise ValueError(f"æ–‡ä»¶åä¸ç¬¦åˆè§„åˆ™: {fname}")
    d = m.groupdict()
    d["lat"] = float(d["lat"])
    d["lon"] = float(d["lon"])
    return d  # {net, sta, lat, lon}

def pick_best_trace(stream):
    """ä¼˜å…ˆé€‰ Z åˆ†é‡ï¼Œé‡‡æ ·ç‡æœ€é«˜çš„ Traceï¼›è‹¥æ—  Zï¼Œåˆ™é€‰é‡‡æ ·ç‡æœ€é«˜çš„"""
    z_traces = [tr for tr in stream if tr.stats.channel.endswith("Z")]
    candidates = z_traces if z_traces else list(stream)
    # é‡‡æ ·ç‡æœ€é«˜è€…
    return max(candidates, key=lambda tr: float(getattr(tr.stats, "sampling_rate", 0.0)))

def to_pandas_time(utc_list):
    """ObsPy UTCDateTime åˆ—è¡¨ -> pandas.Timestamp(UTC)"""
    return pd.to_datetime([t.datetime for t in utc_list], utc=True)

def evenly_sample_per_minute(df, n=30):
    """
    å¯¹å•ä¸ªç«™ç‚¹çš„ (time, amplitude) æŒ‰åˆ†é’Ÿåˆ†ç»„ï¼Œ
    æ¯åˆ†é’Ÿå‡åŒ€é€‰ n ä¸ªæ ·æœ¬ï¼›ç»™æ¯ä¸ªæ ·æœ¬åŠ  slot(0..n-1)
    """
    df = df.sort_values("time")
    df["minute"] = df["time"].dt.floor("min")

    def _sampler(g):
        if len(g) == 0:
            return g
        if len(g) <= n:
            out = g.copy().reset_index(drop=True)
            out["slot"] = np.arange(len(out))
            return out
        idx = np.linspace(0, len(g) - 1, num=n, dtype=int)
        out = g.iloc[idx].copy()
        out["slot"] = np.arange(len(out))
        return out
    try:
        # æ–°ç‰ˆpandaså…¼å®¹æ–¹å¼
        out = df.groupby("minute", group_keys=False).apply(_sampler, include_groups=False)
    except TypeError:
        # æ—§ç‰ˆpandaså…œåº•æ–¹å¼
        out = df.groupby("minute", group_keys=False).apply(_sampler)
    #out = df.groupby("minute", group_keys=False).apply(_sampler)
    # ç¡®ä¿minuteåˆ—æ²¡æœ‰ä¸¢å¤±
    if "minute" not in out.columns and "minute" in df.columns:
        out = out.reset_index()
    return out.reset_index(drop=True)

# ------------------------------
# æ•°æ®åŠ è½½ï¼ˆç¼“å­˜ï¼‰
# ------------------------------
@st.cache_data(show_spinner=True)
def load_one_url(url: str) -> pd.DataFrame:
    meta = parse_meta_from_url(url)
    # ä¸‹è½½
    r = requests.get(url, timeout=60)
    r.raise_for_status()

    # è¯»å– mseedï¼Œé€‰æ‹©æœ€ä½³ trace
    st_obj = read(io.BytesIO(r.content))
    tr = pick_best_trace(st_obj)

    times = to_pandas_time(tr.times("utcdatetime"))
    amps = tr.data.astype(float)

    df = pd.DataFrame({"time": times, "amplitude": amps})
    df["net"] = meta["net"]
    df["sta"] = meta["sta"]
    df["lat"] = meta["lat"]
    df["lon"] = meta["lon"]

    # æ¯åˆ†é’Ÿå‡åŒ€é‡‡æ · 30 ä¸ªç‚¹ï¼ˆä¸è¶³åˆ™å°½å¯èƒ½å¤šï¼‰
    df_s = evenly_sample_per_minute(df, n=30)
    return df_s

@st.cache_data(show_spinner=True)
def load_all(urls):
    parts = []
    for u in urls:
        try:
            parts.append(load_one_url(u))
        except Exception as e:
            st.warning(f"åŠ è½½å¤±è´¥ {u}: {e}")
    if not parts:
        return pd.DataFrame(columns=["time","amplitude","net","sta","lat","lon","minute","slot"])
    df_all = pd.concat(parts, ignore_index=True)

    # æ„é€ å¸§ç´¢å¼•ï¼šæŒ‰ minute å‡åºã€slot å‡åº
    minutes_sorted = np.array(sorted(df_all["minute"].dropna().unique()))
    minute_index = {m: i for i, m in enumerate(minutes_sorted)}
    df_all["minute_idx"] = df_all["minute"].map(minute_index).astype("Int64")
    df_all["frame_idx"] = df_all["minute_idx"] * 30 + df_all["slot"].astype(int)

    # ä¾›å±•ç¤ºä½¿ç”¨çš„å¸§åˆ—è¡¨ï¼ˆå®é™…å­˜åœ¨çš„å¸§ï¼‰
    frames = (
        df_all[["minute", "slot", "frame_idx"]]
        .drop_duplicates()
        .sort_values(["frame_idx"])
        .reset_index(drop=True)
    )

    # åœ°å›¾ä¸­å¿ƒï¼ˆæ‰€æœ‰ç«™ç‚¹çš„ç»çº¬åº¦å‡å€¼ï¼‰
    center_lat = float(df_all["lat"].mean())
    center_lon = float(df_all["lon"].mean())
    coords = df_all[["lat", "lon"]].drop_duplicates().to_numpy()

    return df_all, frames, center_lat, center_lon, coords

# ------------------------------
# é¡µé¢
# ------------------------------
st.set_page_config(page_title="åœ°éœ‡æ³¢å½¢æŸ±çŠ¶å›¾ï¼ˆåŠ¨æ€ï¼‰", layout="wide")
st.title("ğŸŒ æŒ‰æ—¶é—´åŠ¨æ€å±•ç¤ºï¼šå„å°ç«™æŒ¯å¹… 3D æŸ±çŠ¶å›¾")

# å¯é€‰ï¼šæŠŠ URL åˆ—è¡¨å±•ç¤ºå‡ºæ¥ï¼Œä¾¿äºç¡®è®¤
with st.expander("æŸ¥çœ‹ URL åˆ—è¡¨"):
    st.write(url_list)

if not url_list:
    st.info("è¯·å…ˆåœ¨ä»£ç é¡¶éƒ¨å¡«å…¥ mseed æ–‡ä»¶çš„ URL åˆ—è¡¨ã€‚")
    st.stop()

with st.spinner("åŠ è½½ä¸è§£æ mseed æ•°æ®ä¸­â€¦"):
    df_all, frames, center_lat, center_lon, coords = load_all(tuple(url_list))

if df_all.empty or frames.empty:
    st.error("æ²¡æœ‰å¯å±•ç¤ºçš„æ•°æ®ï¼ˆå¯èƒ½æ‰€æœ‰é“¾æ¥éƒ½åŠ è½½å¤±è´¥æˆ–æ–‡ä»¶ä¸ºç©ºï¼‰ã€‚")
    st.stop()

# æ—¶é—´å¸§æ»‘å—ï¼ˆè·¨åˆ†é’Ÿä¸ slotï¼‰
min_frame = int(frames["frame_idx"].min())
max_frame = int(frames["frame_idx"].max())
frame_val = st.slider("é€‰æ‹©æ—¶é—´å¸§ï¼ˆæ¯åˆ†é’Ÿå‡åŒ€ 30 å¸§ï¼‰", min_value=min_frame, max_value=max_frame, value=min_frame, step=1)

# å½“å‰å¸§å¯¹åº”çš„ minute/slot ä¿¡æ¯
row = frames.loc[frames["frame_idx"] == frame_val].iloc[0]
cur_minute = pd.to_datetime(row["minute"])
cur_slot = int(row["slot"])
st.markdown(f"**å½“å‰åˆ†é’Ÿï¼š** {cur_minute}  &nbsp;&nbsp; **å¸§å†… slotï¼š** {cur_slot+1}/30")

# å–å½“å‰å¸§æ•°æ®ï¼ˆè¿™ä¸€åˆ†é’Ÿé‡Œã€è¿™ä¸ª slot çš„æ¯ä¸ªç«™ç‚¹å„ä¸€æ¡ï¼‰
df_now = df_all[(df_all["minute"] == cur_minute) & (df_all["slot"] == cur_slot)].copy()

if df_now.empty:
    st.warning("è¯¥å¸§æ— æ•°æ®ï¼Œè¯•è¯•å…¶å®ƒå¸§ã€‚")
    st.stop()

# å¯è§†åŒ–å­—æ®µï¼šæŸ±é«˜ç”¨ |amplitude|ï¼›é¢œè‰²æŒ‰æ­£è´ŸåŒºåˆ†
df_now["height"] = np.abs(df_now["amplitude"].astype(float))
df_now["r"] = np.where(df_now["amplitude"] >= 0, 255, 30)   # æ­£=åçº¢ï¼Œè´Ÿ=åè“
df_now["g"] = 50
df_now["b"] = np.where(df_now["amplitude"] >= 0, 60, 255)

# åŠ¨æ€é«˜åº¦ç¼©æ”¾ï¼šè®© 99 åˆ†ä½é«˜åº¦ ~ 1000m
p99 = float(df_now["height"].quantile(0.99)) if len(df_now) > 1 else float(df_now["height"].max())
elev_scale = 1000.0

# åŠå¾„ï¼ˆç±³ï¼‰ï¼šå…¨å±€åœ°ç†èŒƒå›´å¤§æ—¶å¯é€‚å½“åŠ å¤§
radius_m = 100000

tooltip = {
    "html": "<b>{net}.{sta}</b><br/>amp: {amplitude}<br/>lat: {lat}, lon: {lon}<br/>{minute} slot {slot}",
    "style": {"color": "white"},
}

layer = pdk.Layer(
    "ColumnLayer",
    data=df_now,
    get_position="[lon, lat]",     # æ³¨æ„é¡ºåºï¼šç»åº¦åœ¨å‰
    get_elevation="height",
    elevation_scale=elev_scale,     # è°ƒæ•´é«˜åº¦
    radius=radius_m,
    get_fill_color="[r, g, b, 180]",
    pickable=True,
    auto_highlight=True,
)

# è§†è§’ï¼šä»¥æ‰€æœ‰ç«™ç‚¹ä¸­å¿ƒä¸ºåŸºå‡†
view_state = pdk.ViewState(
    latitude=center_lat,
    longitude=center_lon,
    zoom=2,
    pitch=45,
    bearing=0,
)

# ä½¿ç”¨ map_style=Noneï¼ˆä¸éœ€è¦ tokenï¼‰
deck = pdk.Deck(
    layers=[layer],
    initial_view_state=view_state,
    tooltip=tooltip,
    map_style=None,
)

st.pydeck_chart(deck, use_container_width=True)

with st.expander("æ•°æ®åˆ‡ç‰‡ï¼ˆå½“å‰å¸§ï¼‰"):
    st.dataframe(
        df_now[["net","sta","lat","lon","time","amplitude","minute","slot"]]
        .sort_values(["net","sta"])
        .reset_index(drop=True)
    )

st.caption("æç¤ºï¼šæŸ±é«˜ä¸º |amplitude|ï¼Œé¢œè‰²åŒºåˆ†æ­£è´Ÿï¼›è‹¥åœ°å›¾ä¸æ˜¾ç¤ºï¼Œè¯·ç¡®ä¿ URL å¯è®¿é—®ï¼Œä¸”æ–‡ä»¶ååŒ…å«ç»çº¬åº¦ã€‚")
